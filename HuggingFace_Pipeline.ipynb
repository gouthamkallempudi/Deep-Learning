{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9cXZwn7jgqKMqhaSo2Wu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gouthamkallempudi/Deep-Learning/blob/master/HuggingFace_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "niYy0YnqM71P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LsAgpQAUL-kR"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start with sentiment analysis"
      ],
      "metadata": {
        "id": "IjtJ6SaxMYUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline(\n",
        "    task = \"sentiment-analysis\",\n",
        "    model = \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device = 0)\n",
        "\n",
        "print(nlp(\"I love this movie!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v37HXT5MSx_",
        "outputId": "4698981d-f214-4627-9a53-7714b4a00d1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998775720596313}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "SFlJd93GNZ24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "ner = pipeline(\n",
        "    task = \"ner\",\n",
        "    model = \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
        "    config = config,\n",
        "    device = 0\n",
        ")\n",
        "\n",
        "print(ner(\"Hugging face Inc. is based in New York.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgtiKFreMzMT",
        "outputId": "3b88f027-4c34-467b-a436-9af1ed44c24a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'I-ORG', 'score': np.float32(0.98959446), 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}, {'entity': 'I-ORG', 'score': np.float32(0.8172372), 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}, {'entity': 'I-ORG', 'score': np.float32(0.9829742), 'index': 3, 'word': 'face', 'start': 8, 'end': 12}, {'entity': 'I-ORG', 'score': np.float32(0.99925417), 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}, {'entity': 'I-LOC', 'score': np.float32(0.9987984), 'index': 9, 'word': 'New', 'start': 30, 'end': 33}, {'entity': 'I-LOC', 'score': np.float32(0.9988224), 'index': 10, 'word': 'York', 'start': 34, 'end': 38}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbpzeB2bOWNd",
        "outputId": "5f9dee34-a7a2-4508-ef48-6f931249d3b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill Mask Task"
      ],
      "metadata": {
        "id": "ZBbPn6npQwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    task = \"fill-mask\",\n",
        "    model = \"bert-base-cased\",\n",
        "    tokenizer = tokenizer,\n",
        "    device = 0\n",
        ")\n",
        "\n",
        "print(fill_mask(\"The Capital of Germany is [MASK].\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZFGxI6wQVnF",
        "outputId": "0b8e9eba-c03f-4a62-a748-139f8d79ec03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.1705125868320465, 'token': 3206, 'token_str': 'Berlin', 'sequence': 'The Capital of Germany is Berlin.'}, {'score': 0.09991458803415298, 'token': 12312, 'token_str': 'Cologne', 'sequence': 'The Capital of Germany is Cologne.'}, {'score': 0.08590444177389145, 'token': 8339, 'token_str': 'Hamburg', 'sequence': 'The Capital of Germany is Hamburg.'}, {'score': 0.07850676774978638, 'token': 9529, 'token_str': 'Frankfurt', 'sequence': 'The Capital of Germany is Frankfurt.'}, {'score': 0.06017882004380226, 'token': 13269, 'token_str': 'Stuttgart', 'sequence': 'The Capital of Germany is Stuttgart.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer → for text\n",
        "\n",
        "feature_extractor → for images/audio"
      ],
      "metadata": {
        "id": "lr_2lUEiRkDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Classification"
      ],
      "metadata": {
        "id": "OrnM6sDCRmMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Load the feature extractor (optional)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "image_classifier = pipeline(\n",
        "    task=\"image-classification\",\n",
        "    model=\"google/vit-base-patch16-224\",\n",
        "    feature_extractor=feature_extractor,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "print(image_classifier(image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXFycbEDRNmq",
        "outputId": "0637cc1e-4160-4d3b-ad31-77859b915c35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'tabby, tabby cat', 'score': 0.27686837315559387}, {'label': 'tiger cat', 'score': 0.2763683497905731}, {'label': 'Egyptian cat', 'score': 0.14028170704841614}, {'label': 'hay', 'score': 0.0253145769238472}, {'label': 'wool, woolen, woollen', 'score': 0.019932707771658897}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "# Load image\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Load image processor\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "image_pipe = pipeline(\n",
        "    task=\"image-classification\",\n",
        "    model=\"google/vit-base-patch16-224\",\n",
        "    image_processor=image_processor,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "print(image_pipe(image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkipADF1SN-m",
        "outputId": "c182a551-6369-486d-c093-e49069d3a20c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'tabby, tabby cat', 'score': 0.27686837315559387}, {'label': 'tiger cat', 'score': 0.2763683497905731}, {'label': 'Egyptian cat', 'score': 0.14028170704841614}, {'label': 'hay', 'score': 0.0253145769238472}, {'label': 'wool, woolen, woollen', 'score': 0.019932707771658897}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal Models\n",
        "\n",
        "Using processor arguments\n",
        "\n",
        "Text + images (like BLIP, Flamingo, Donut)\n",
        "\n",
        "Audio + text (like Whisper)\n",
        "\n",
        "Text + speech + vision (like some MetaAI and Google models)\n",
        "\n"
      ],
      "metadata": {
        "id": "ErIAxbkMS_Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image to text\n",
        "\n",
        "from transformers import BlipProcessor\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "image_to_text = pipeline(\n",
        "    task=\"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-base\",\n",
        "    processor=processor,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "print(image_to_text(image))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaH_uEdDS2R2",
        "outputId": "ca14e73b-898e-46cd-a4ed-a0e9d43c69d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'a cat laying on a pile of yarn'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asr_pipeline = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\",\n",
        "    device=0  # Use GPU\n",
        ")\n",
        "\n",
        "# Load an audio file (must be .wav or supported format)\n",
        "# You can replace this with a local file path if needed\n",
        "audio_file = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
        "\n",
        "# Run speech-to-text\n",
        "print(asr_pipeline(audio_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQWUDK0-Tqol",
        "outputId": "cc7857da-84ac-455c-8893-3dbd6a060e0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQlDkjloUDUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Visual Q&A and Captioning Tool\"\n",
        "\n",
        "Accepts an image from the user.\n",
        "\n",
        "Generates:\n",
        "\n",
        "A caption (image-to-text).\n",
        "\n",
        "Answers to a textual question about the image (visual question answering / VQA)."
      ],
      "metadata": {
        "id": "7rft7K2PUrjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load models\n",
        "caption_pipeline = pipeline(\n",
        "    task=\"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-base\",\n",
        "    device=0\n",
        ")\n",
        "\n",
        "vqa_pipeline = pipeline(\n",
        "    task=\"vqa\",\n",
        "    model=\"dandelin/vilt-b32-finetuned-vqa\",\n",
        "    device=0\n",
        ")\n",
        "\n",
        "# Load image\n",
        "image_path = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Generate caption\n",
        "caption_result = caption_pipeline(image)\n",
        "caption = caption_result[0]['generated_text']\n",
        "print(f\"\\n🖼️  Caption: {caption}\")\n",
        "\n",
        "# Ask user question about image\n",
        "question = input(\"\\n❓ Ask a question about this image: \")\n",
        "\n",
        "# Run VQA\n",
        "vqa_result = vqa_pipeline(image, question=question)\n",
        "answer = vqa_result[0]['answer']\n",
        "print(f\"\\n🤖 Answer: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7I4RQ-bUwMd",
        "outputId": "fbd47a0a-ccdd-4f81-f6ba-5ce13ec3db2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️  Caption: a cat laying on a pile of yarn\n",
            "\n",
            "❓ Ask a question about this image: whats the color of the cat\n",
            "\n",
            "🤖 Answer: gray and white\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3T_zLMjVOTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}